# -*- coding: utf-8 -*-
"""ECE 219_Project 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z_voMuzt7EwcR5BTQNUUFSBwT6BGfmvO

#Importing Data
"""

import itertools
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
def plot_mat(mat, xticklabels = None, yticklabels = None, pic_fname = None, size=(-1,-1), if_show_values = True,
             colorbar = True, grid = 'k', xlabel = None, ylabel = None, title = None, vmin=None, vmax=None):
    if size == (-1, -1):
        size = (mat.shape[1] / 3, mat.shape[0] / 3)

    fig = plt.figure(figsize=size)
    ax = fig.add_subplot(1,1,1)

    # im = ax.imshow(mat, cmap=plt.cm.Blues)
    im = ax.pcolor(mat, cmap=plt.cm.Blues, linestyle='-', linewidth=0.5, edgecolor=grid, vmin=vmin, vmax=vmax)
    
    if colorbar:
        plt.colorbar(im,fraction=0.046, pad=0.06)
    # tick_marks = np.arange(len(classes))
    # Ticks
    lda_num_topics = mat.shape[0]
    nmf_num_topics = mat.shape[1]
    yticks = np.arange(lda_num_topics)
    xticks = np.arange(nmf_num_topics)
    ax.set_xticks(xticks + 0.5)
    ax.set_yticks(yticks + 0.5)
    if xticklabels is None:
        xticklabels = [str(i) for i in xticks]
    if yticklabels is None:
        yticklabels = [str(i) for i in yticks]
    ax.set_xticklabels(xticklabels)
    ax.set_yticklabels(yticklabels)

    # Minor ticks
    # ax.set_xticks(xticks, minor=True);
    # ax.set_yticks(yticks, minor=True);
    # ax.set_xticklabels([], minor=True)
    # ax.set_yticklabels([], minor=True)

    # ax.grid(which='minor', color='k', linestyle='-', linewidth=0.5)

    # tick labels on all four sides
    ax.tick_params(labelright = True, labeltop = False)

    if ylabel:
        plt.ylabel(ylabel, fontsize=15)
    if xlabel:
        plt.xlabel(xlabel, fontsize=15)
    if title:
        plt.title(title, fontsize=15)

    # im = ax.imshow(mat, interpolation='nearest', cmap=plt.cm.Blues)
    ax.invert_yaxis()

    # thresh = mat.max() / 2

    def show_values(pc, fmt="%.3f", **kw):
        pc.update_scalarmappable()
        ax = pc.axes
        for p, color, value in itertools.zip_longest(pc.get_paths(), pc.get_facecolors(), pc.get_array()):
            x, y = p.vertices[:-2, :].mean(0)
            if np.all(color[:3] > 0.5):
                color = (0.0, 0.0, 0.0)
            else:
                color = (1.0, 1.0, 1.0)
            ax.text(x, y, fmt % value, ha="center", va="center", color=color, **kw, fontsize=10)

    if if_show_values:
        show_values(im)
    # for i, j in itertools.product(range(mat.shape[0]), range(mat.shape[1])):
    #     ax.text(j, i, "{:.2f}".format(mat[i, j]), fontsize = 4,
    #              horizontalalignment="center",
    #              color="white" if mat[i, j] > thresh else "black")

    plt.tight_layout()
    if pic_fname:
        plt.savefig(pic_fname, dpi=300, transparent=True)
    plt.show()
    plt.close()

import numpy as np
import pandas as pd

from sklearn.datasets import fetch_20newsgroups
class1 = ['comp.graphics', 'comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware','comp.sys.mac.hardware']
class2 = ['rec.autos','rec.motorcycles','rec.sport.baseball','rec.sport.hockey']
twenty_class1 = fetch_20newsgroups(categories=class1, remove = ('headers','footers'), random_state=42)
twenty_class2 = fetch_20newsgroups(categories=class2, remove = ('headers','footers'), random_state=42)

from pprint import pprint
pprint(list(twenty_class1.target_names))

pprint(list(twenty_class2.target_names))

print(twenty_class1.keys())

print(len(twenty_class1.data)) 
print(len(twenty_class2.data)) 
print(len(twenty_class1.target_names))
print(len(twenty_class2.target_names))

class1Data = twenty_class1['data']
class2Data = twenty_class2['data']

class1DF = pd.DataFrame(class1Data, columns=['data'])
class1DF['target'] = np.ones(len(class1Data))

class2DF = pd.DataFrame(class2Data, columns=['data'])
class2DF['target'] = np.ones(len(class2Data))*2

print(len(class1DF['data']))
print(len(class2DF['data']))

df = pd.concat([class1DF,class2DF],axis=0,ignore_index=True) # concatenate along rows

df.shape

df.dtypes

print(df.data[0])

labels = df.target

"""**Data to use is: df
Columns: 'data' (document), 'target' (class)**

# Question 1
"""

# transform documents into TF-IDF vectors
    # min_df = 3
    # exclude stopwords
    # remove headers and footers #TODO: STILL NEED TO REMOVE HEADER AND FOOTER
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df=3, stop_words='english')
vectors = vectorizer.fit_transform(df.data.values)

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer()
X_tfidfVectors = tfidf_transformer.fit_transform(vectors)

print("TF-IDF matrix dimensions: " + str(X_tfidfVectors.shape))

"""# Question 2 and 3"""

from sklearn import metrics
from time import time
from collections import defaultdict
import seaborn as sns
import matplotlib.pyplot as plt

evaluations = []
evaluations_std = []

def fit_and_evaluate(km, X, name=None, n_runs=5):
  name = km.__class__.__name__ if name is None else name

  train_times=[]
  scores=defaultdict(list)
  for seed in range(n_runs):
    km.set_params(random_state=seed)
    t0=time()
    km.fit(X)
    train_times = np.append(train_times, time() - t0)


    scores["Homogeneity"].append(metrics.homogeneity_score(labels,km.labels_))
    scores["Completeness"].append(metrics.completeness_score(labels, km.labels_))
    scores["V-measure"].append(metrics.v_measure_score(labels, km.labels_))
    scores["Adjusted Rand-Index"].append(metrics.adjusted_rand_score(labels, km.labels_))
    scores["Adjusted Mutual Information"].append(metrics.adjusted_mutual_info_score(labels, km.labels_))

    train_times = np.asarray(train_times)
    print(f"\nclustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s ")
    
    evaluation = {
        "estimator": name,
        "train_time": train_times.mean(),
    }

    evaluation_std = {
        "estimator": name,
        "train_time": train_times.std(),
    }

    for score_name, score_values in scores.items():
        mean_score, std_score = np.mean(score_values), np.std(score_values)
        print(f"{score_name}: {mean_score:.3f} ± {std_score:.3f}")
        evaluation[score_name] = mean_score
        evaluation_std[score_name] = std_score
    evaluations.append(evaluation)
    evaluations_std.append(evaluation_std)

    contingencyMatrix = metrics.cluster.contingency_matrix(labels, km.labels_)
    ax = plt.axes()
    s = sns.heatmap(contingencyMatrix, annot=True, cmap='Blues', ax=ax)
    ax.set_title("Contingency Matrix for KMeans")
    plt.show()

# K-means clustering
    # k = 2
    # use the TF-IDF data
    # random_state = 0, max_iter>=1000, n_init>=30

# Obtain clustering results and ground truth labels
# contingency table A
    # A_ij = # of data points that belond to the ith class and the jth cluster

# report contingency table
# use plotmat.py to visualize the matrix
# report clustering measurements: homogeneity, completeness, v-measure, adjusted rand index, adjusted mutual information score

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, random_state=0, max_iter=1000, n_init=30)
fit_and_evaluate(kmeans,X_tfidfVectors, name="KMeans\non tf-idf vectors", n_runs=3)

"""# Question 4"""

# see what ratio of the variance of the original data is retained after dimensionality reduction
# create truncated SVD representation of TF-IDF matrix
from sklearn.decomposition import TruncatedSVD

principleComponents = [1, 10, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]
explainedVariance = np.zeros(12)

i = 0

for r in principleComponents:
  svd = TruncatedSVD(n_components=r)
  svd.fit_transform(X_tfidfVectors)
  explainedVariance[i] = svd.explained_variance_ratio_.sum()
  i = i+1

# report plot of percentage of variance that the top r principle components retain vs. r (for r=1 to 1000)
plt.plot(principleComponents,explainedVariance)
plt.xlabel("# principle components (r)")
plt.ylabel("% of Explained Variance")
plt.title("Explained Variance for top r principle components")
plt.show()

"""another way of coding SVD"""

# see what ratio of the variance of the original data is retained after dimensionality reduction
# create truncated SVD representation of TF-IDF matrix
from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=1000)
svd.fit_transform(X_tfidfVectors)
principleComponents = list(range(1,1001,50))
i=0

explainedVariance = np.zeros(len(principleComponents))
for r in principleComponents:
  for k in range (0,r):
    explainedVariance[i] = explainedVariance[i]+svd.explained_variance_ratio_[k]
  i = i+1
  print(r)

# report plot of percentage of variance that the top r principle components retain vs. r (for r=1 to 1000)
plt.plot(principleComponents,explainedVariance)
plt.xlabel("# principle components (r)")
plt.ylabel("% of Explained Variance")
plt.title("Explained Variance for top r principle components")
plt.show()

"""# Question 5, 6, and 7

##Truncated SVD/PCA
"""

# choose the dimension that yields the best result in terms of clustering purity metrics
# r = n_components
# r = 1, 10, 20, 50, 100, 300

principleComponents = [1, 10, 20, 50, 100, 300]

svd = TruncatedSVD(n_components=300)
X_svd = svd.fit_transform(X_tfidfVectors)
print(X_svd.shape)

kmeans = KMeans(n_clusters=2, random_state=0, max_iter=1000, n_init=30)

for r in principleComponents:
  X_svd_r = X_svd[0:, 0:r]
  print(r)
  print(X_svd_r.shape)
  fit_and_evaluate(kmeans,X_svd_r, name="KMeans\non tf-idf vectors", n_runs=1)

principleComponents = [1, 10, 20, 50, 100, 300]

svd = TruncatedSVD(n_components=300)
X_svd = svd.fit_transform(X_tfidfVectors)
homogeneity = []
completeness = []
vMeasure = []
adjustedRand = []
adjustedMutual = []

kmeans = KMeans(n_clusters=2, random_state=0, max_iter=1000, n_init=30)

for r in principleComponents:
  X_svd_r = X_svd[0:, 0:r]
  print(r)
  print(X_svd_r.shape)
  kmeans.fit(X_svd_r)
  homogeneity.append(metrics.homogeneity_score(labels,kmeans.labels_))
  completeness.append(metrics.completeness_score(labels, kmeans.labels_))
  vMeasure.append(metrics.v_measure_score(labels, kmeans.labels_))
  adjustedRand.append(metrics.adjusted_rand_score(labels, kmeans.labels_))
  adjustedMutual.append(metrics.adjusted_mutual_info_score(labels, kmeans.labels_))

# report plot of percentage of variance that the top r principle components retain vs. r (for r=1 to 1000)
plt.plot(principleComponents,homogeneity)
plt.xlabel("# principle components (r)")
plt.ylabel("Homogeneity Score")
plt.title("Homogeneity vs. # Prinicple Components (SVD)")
plt.show()

plt.plot(principleComponents,completeness)
plt.xlabel("# principle components (r)")
plt.ylabel("Completeness Score")
plt.title("Completeness vs. # Prinicple Components (SVD)")
plt.show()

plt.plot(principleComponents,vMeasure)
plt.xlabel("# principle components (r)")
plt.ylabel("V Measure Score")
plt.title("V Measure vs. # Prinicple Components (SVD)")
plt.show()

plt.plot(principleComponents,adjustedRand)
plt.xlabel("# principle components (r)")
plt.ylabel("Adjusted Rand Score")
plt.title("Adjusted Rand Score vs. # Prinicple Components (SVD)")
plt.show()

plt.plot(principleComponents,adjustedMutual)
plt.xlabel("# principle components (r)")
plt.ylabel("Adjusted Mutual Info Score")
plt.title("Adjusted Mutual Info Score vs. # Prinicple Components (SVD)")
plt.show()

print(principleComponents)
print(adjustedMutual)

"""##NMF"""

from sklearn.decomposition import NMF

principleComponents = [1, 10, 20, 50, 100, 300]

homogeneity = []
completeness = []
vMeasure = []
adjustedRand = []
adjustedMutual = []

kmeans = KMeans(n_clusters=2, random_state=0, max_iter=1000, n_init=30)

for r in principleComponents:
  nmf = NMF(n_components=r)
  X_nmf = nmf.fit_transform(X_tfidfVectors)
  
  print(r)
  print(X_nmf.shape)
  kmeans.fit(X_nmf)

  homogeneity.append(metrics.homogeneity_score(labels,kmeans.labels_))
  completeness.append(metrics.completeness_score(labels, kmeans.labels_))
  vMeasure.append(metrics.v_measure_score(labels, kmeans.labels_))
  adjustedRand.append(metrics.adjusted_rand_score(labels, kmeans.labels_))
  adjustedMutual.append(metrics.adjusted_mutual_info_score(labels, kmeans.labels_))

# report plot of percentage of variance that the top r principle components retain vs. r (for r=1 to 1000)
plt.plot(principleComponents,homogeneity)
plt.xlabel("# principle components (r)")
plt.ylabel("Homogeneity Score")
plt.title("Homogeneity vs. # Prinicple Components (NMF)")
plt.show()

plt.plot(principleComponents,completeness)
plt.xlabel("# principle components (r)")
plt.ylabel("Completeness Score")
plt.title("Completeness vs. # Prinicple Components (NMF)")
plt.show()

plt.plot(principleComponents,vMeasure)
plt.xlabel("# principle components (r)")
plt.ylabel("V Measure Score")
plt.title("V Measure vs. # Prinicple Components (NMF)")
plt.show()

plt.plot(principleComponents,adjustedRand)
plt.xlabel("# principle components (r)")
plt.ylabel("Adjusted Rand Score")
plt.title("Adjusted Rand Score vs. # Prinicple Components (NMF)")
plt.show()

plt.plot(principleComponents,adjustedMutual)
plt.xlabel("# principle components (r)")
plt.ylabel("Adjusted Mutual Info Score")
plt.title("Adjusted Mutual Info Score vs. # Prinicple Components (NMF)")
plt.show()

"""**Optimal choice of r:**

SVD: 20

NMF: 20

# Question 8 and 9

X_tfidfVectors 
labels

SVD = 20
NMF = 20

## SVD
"""

# visualize the clustering results
# create dense representations
# project these representations into a 2-D plane for visualization

svd = TruncatedSVD(n_components=20)
X_svd = svd.fit_transform(X_tfidfVectors)
print(X_svd.shape)

kmeans = KMeans(n_clusters=2, random_state=0, max_iter=1000, n_init=30)
Y_pred_svd = kmeans.fit_predict(X_svd)

Y_pred_svd.shape

plt.scatter(X_svd[:,0],X_svd[:,1],c=Y_pred_svd)
plt.scatter(kmeans.cluster_centers_[:, 0], 
            kmeans.cluster_centers_[:, 1], 
            s=50,                             # Set centroid size
            c='red') 
plt.title("SVD Cluster with Clustering Labels") 
plt.show()

plt.scatter(X_svd[:,0],X_svd[:,1],c=labels)
plt.scatter(kmeans.cluster_centers_[:, 0], 
            kmeans.cluster_centers_[:, 1], 
            s=50,                             # Set centroid size
            c='red')  
plt.title("SVD Cluster with Ground Truth Labels") 
plt.show()

"""## NMF"""

# visualize the clustering results
# create dense representations
# project these representations into a 2-D plane for visualization

nmf = NMF(n_components=20)
X_nmf = nmf.fit_transform(X_tfidfVectors)
print(X_nmf.shape)

kmeans = KMeans(n_clusters=2, random_state=0, max_iter=1000, n_init=30)
Y_pred_nmf = kmeans.fit_predict(X_nmf)

plt.scatter(X_nmf[:,0],X_nmf[:,1],c=Y_pred_nmf)
plt.scatter(kmeans.cluster_centers_[:, 0], 
            kmeans.cluster_centers_[:, 1], 
            s=50,                             # Set centroid size
            c='red') 
plt.title("NMF Cluster with Clustering Labels") 
plt.show()

plt.scatter(X_nmf[:,0],X_nmf[:,1],c=labels)
plt.scatter(kmeans.cluster_centers_[:, 0], 
            kmeans.cluster_centers_[:, 1], 
            s=50,                             # Set centroid size
            c='red')  
plt.title("NMF Cluster with Ground Truth Labels") 
plt.show()

"""#Question 10"""

import numpy as np
from scipy.optimize import linear_sum_assignment

evaluations = []
evaluations_std = []

def fit_and_evaluate(km, X, name=None, n_runs=5):
  name = km.__class__.__name__ if name is None else name

  train_times=[]
  scores=defaultdict(list)
  for seed in range(n_runs):
    km.set_params(random_state=seed)
    t0=time()
    km.fit(X)
    train_times = np.append(train_times, time() - t0)


    scores["Homogeneity"].append(metrics.homogeneity_score(labels,km.labels_))
    scores["Completeness"].append(metrics.completeness_score(labels, km.labels_))
    scores["V-measure"].append(metrics.v_measure_score(labels, km.labels_))
    scores["Adjusted Rand-Index"].append(metrics.adjusted_rand_score(labels, km.labels_))
    scores["Adjusted Mutual Information"].append(metrics.adjusted_mutual_info_score(labels, km.labels_))

    train_times = np.asarray(train_times)
    print(f"\nclustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s ")
    
    evaluation = {
        "estimator": name,
        "train_time": train_times.mean(),
    }

    evaluation_std = {
        "estimator": name,
        "train_time": train_times.std(),
    }

    for score_name, score_values in scores.items():
        mean_score, std_score = np.mean(score_values), np.std(score_values)
        print(f"{score_name}: {mean_score:.3f} ± {std_score:.3f}")
        evaluation[score_name] = mean_score
        evaluation_std[score_name] = std_score
    evaluations.append(evaluation)
    evaluations_std.append(evaluation_std)

    contingencyMatrix = metrics.cluster.contingency_matrix(labels, km.labels_)
    rows, cols = linear_sum_assignment(contingencyMatrix,maximize=True)
    plot_mat(contingencyMatrix[rows[:,np.newaxis], cols], xticklabels=cols, yticklabels=rows, size=(15,15))

"""## Loading documents (20)

twentyDF,
labels
"""

# load documents for all 20 categories
from sklearn.datasets import fetch_20newsgroups
newsgroups = fetch_20newsgroups(remove = ('headers','footers'), random_state=42)

print(len(newsgroups.data))

newsgroups.target

twentyDF=pd.DataFrame(newsgroups['data'], columns = ['data'])
twentyDF['labels']=newsgroups['target']

twentyDF.head

labels = twentyDF.labels

"""## Construct TF-IDF"""

# transform documents into TF-IDF vectors
    # min_df = 3
    # exclude stopwords
    # remove headers and footers #TODO: STILL NEED TO REMOVE HEADER AND FOOTER
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df=3, stop_words='english')
vectors = vectorizer.fit_transform(twentyDF.data.values)

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer()
X_tfidfVectors = tfidf_transformer.fit_transform(vectors)

print("TF-IDF matrix dimensions: " + str(X_tfidfVectors.shape))

"""##SVD

X_tfidfVectors, labels
"""

# choosing the best amount of principle components
principleComponents = [1, 10, 20, 50, 100, 300]

svd = TruncatedSVD(n_components=300)
X_svd = svd.fit_transform(X_tfidfVectors)
print(X_svd.shape)

kmeans = KMeans(n_clusters=20, random_state=0, max_iter=1000, n_init=30)

for r in principleComponents:
  X_svd_r = X_svd[0:, 0:r]
  print(r)
  print(X_svd_r.shape)
  fit_and_evaluate(kmeans,X_svd_r, name="KMeans\non tf-idf vectors", n_runs=1)

# reduce dimensionality using SVD, choose n_components = 300
svd = TruncatedSVD(n_components=300)
X_svd = svd.fit_transform(X_tfidfVectors)

# k-means clustering (k=20)
kmeans = KMeans(n_clusters=20, random_state=0, max_iter=1000, n_init=30)
# visualize contingency matrix 
# report the five clustering metrics
fit_and_evaluate(kmeans,X_svd, name="KMeans\non tf-idf vectors", n_runs=1)

# visualize the clustering results
kmeans = KMeans(n_clusters=20, random_state=0, max_iter=1000, n_init=30)
Y_pred_svd = kmeans.fit_predict(X_svd)

plt.scatter(X_svd[:,0],X_svd[:,1],c=Y_pred_svd)
plt.scatter(kmeans.cluster_centers_[:, 0], 
            kmeans.cluster_centers_[:, 1], 
            s=50,                             # Set centroid size
            c='red') 
plt.title("SVD Cluster with Clustering Labels") 
plt.show()

plt.scatter(X_svd[:,0],X_svd[:,1],c=labels)
plt.scatter(kmeans.cluster_centers_[:, 0], 
            kmeans.cluster_centers_[:, 1], 
            s=50,                             # Set centroid size
            c='red')  
plt.title("SVD Cluster with Ground Truth Labels") 
plt.show()

"""##NMF"""

# choosing the best amount of principle components
from sklearn.decomposition import NMF

principleComponents = [1, 10, 20, 50, 100]

homogeneity = []
completeness = []
vMeasure = []
adjustedRand = []
adjustedMutual = []

kmeans = KMeans(n_clusters=20, random_state=0, max_iter=1000, n_init=30)

for r in principleComponents:
  print(r)
  nmf = NMF(n_components=r)
  X_nmf = nmf.fit_transform(X_tfidfVectors)
  fit_and_evaluate(kmeans,X_nmf, name="KMeans\non tf-idf vectors", n_runs=1)

# reduce dimensionality using NMF, choose n_components = 20
nmf = NMF(n_components=20)
X_nmf = nmf.fit_transform(X_tfidfVectors)

# k-means clustering (k=20)
kmeans = KMeans(n_clusters=20, random_state=0, max_iter=1000, n_init=30)
# visualize contingency matrix 
# report the five clustering metrics
fit_and_evaluate(kmeans,X_nmf, name="KMeans\non tf-idf vectors", n_runs=1)

# visualize the clustering results
kmeans = KMeans(n_clusters=20, random_state=0, max_iter=1000, n_init=30)
Y_pred_nmf = kmeans.fit_predict(X_nmf)

plt.scatter(X_nmf[:,0],X_nmf[:,1],c=Y_pred_nmf)
plt.scatter(kmeans.cluster_centers_[:, 0], 
            kmeans.cluster_centers_[:, 1], 
            s=50,                             # Set centroid size
            c='red') 
plt.title("NMF Cluster with Clustering Labels") 
plt.show()

plt.scatter(X_nmf[:,0],X_nmf[:,1],c=labels)
plt.scatter(kmeans.cluster_centers_[:, 0], 
            kmeans.cluster_centers_[:, 1], 
            s=50,                             # Set centroid size
            c='red')  
plt.title("NMF Cluster with Ground Truth Labels") 
plt.show()

"""#Question 11, 12, and 13

X_tfidfVectors, labels
"""

!pip install umap-learn

# UMAP dimensionality reductions
    # n_components = [5,20,200]
    # metric = "cosine" vs. "euclidean"
import umap.umap_ as umap

"""## cosine metric"""

kmeans = KMeans(n_clusters=20, random_state=0, max_iter=1000, n_init=30)

n_components = [5,20,200]
for dimension in n_components:
  print("# Components: " + str(dimension))
  reducer = umap.UMAP(n_components=dimension, metric='cosine')
  X_cosine_umap = reducer.fit_transform(X_tfidfVectors)
  fit_and_evaluate(kmeans,X_cosine_umap, name="KMeans\non tf-idf vectors", n_runs=1)

"""## euclidean metric"""

kmeans = KMeans(n_clusters=20, random_state=0, max_iter=1000, n_init=30)

n_components = [5,20,200]
for dimension in n_components:
  print("# Components: " + str(dimension))
  reducer = umap.UMAP(n_components=dimension, metric='euclidean')
  X_euclidean_umap = reducer.fit_transform(X_tfidfVectors)
  fit_and_evaluate(kmeans,X_euclidean_umap, name="KMeans\non tf-idf vectors", n_runs=1)

"""# Question 14"""

# Agglomerative clustering
    # linkage criteria = ['ward', 'single']
    # n_clusters = 20
from sklearn.cluster import AgglomerativeClustering

reducer = umap.UMAP(n_components=20, metric='cosine')
X_reduced = reducer.fit_transform(X_tfidfVectors)
linkageCriteria = ['ward', 'single']

for linkage in linkageCriteria:
  ag = AgglomerativeClustering(n_clusters=20,linkage=linkage)
  print("Linkage Criteria: " + str(linkage))
  ag.fit(X_reduced)

  print("Homogeneity: " + str(metrics.homogeneity_score(labels,ag.labels_)))
  print("Completeness: " + str(metrics.completeness_score(labels, ag.labels_)))
  print("V Measure: " + str(metrics.v_measure_score(labels, ag.labels_)))
  print("Adjusted Rand: " + str(metrics.adjusted_rand_score(labels, ag.labels_)))
  print("Adjusted Mutual Info: " + str(metrics.adjusted_mutual_info_score(labels, ag.labels_)))

"""# Question 15"""

!pip install hdbscan

# HDBSCAN
    # min_cluster_size = [20,100,200]
import hdbscan

reducer = umap.UMAP(n_components=20, metric='cosine')
X_reduced = reducer.fit_transform(X_tfidfVectors)
minCluster = [20,100,200]

for min_cluster_size in minCluster:

  hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)
  print("\nmin_cluster_size: " + str(min_cluster_size))
  hdb.fit(X_reduced)

  print("Homogeneity: " + str(metrics.homogeneity_score(labels,hdb.labels_)))
  print("Completeness: " + str(metrics.completeness_score(labels, hdb.labels_)))
  print("V Measure: " + str(metrics.v_measure_score(labels, hdb.labels_)))
  print("Adjusted Rand: " + str(metrics.adjusted_rand_score(labels, hdb.labels_)))
  print("Adjusted Mutual Info: " + str(metrics.adjusted_mutual_info_score(labels, hdb.labels_)))

"""##Plot Mat"""

import itertools
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
def plot_mat(mat, xticklabels = None, yticklabels = None, pic_fname = None, size=(-1,-1), if_show_values = True,
             colorbar = True, grid = 'k', xlabel = None, ylabel = None, title = None, vmin=None, vmax=None):
    if size == (-1, -1):
        size = (mat.shape[1] / 3, mat.shape[0] / 3)

    fig = plt.figure(figsize=size)
    ax = fig.add_subplot(1,1,1)

    # im = ax.imshow(mat, cmap=plt.cm.Blues)
    im = ax.pcolor(mat, cmap=plt.cm.Blues, linestyle='-', linewidth=0.5, edgecolor=grid, vmin=vmin, vmax=vmax)
    
    if colorbar:
        plt.colorbar(im,fraction=0.046, pad=0.06)
    # tick_marks = np.arange(len(classes))
    # Ticks
    lda_num_topics = mat.shape[0]
    nmf_num_topics = mat.shape[1]
    yticks = np.arange(lda_num_topics)
    xticks = np.arange(nmf_num_topics)
    ax.set_xticks(xticks + 0.5)
    ax.set_yticks(yticks + 0.5)
    if xticklabels is None:
        xticklabels = [str(i) for i in xticks]
    if yticklabels is None:
        yticklabels = [str(i) for i in yticks]
    ax.set_xticklabels(xticklabels)
    ax.set_yticklabels(yticklabels)

    # Minor ticks
    # ax.set_xticks(xticks, minor=True);
    # ax.set_yticks(yticks, minor=True);
    # ax.set_xticklabels([], minor=True)
    # ax.set_yticklabels([], minor=True)

    # ax.grid(which='minor', color='k', linestyle='-', linewidth=0.5)

    # tick labels on all four sides
    ax.tick_params(labelright = True, labeltop = False)

    if ylabel:
        plt.ylabel(ylabel, fontsize=15)
    if xlabel:
        plt.xlabel(xlabel, fontsize=15)
    if title:
        plt.title(title, fontsize=15)

    # im = ax.imshow(mat, interpolation='nearest', cmap=plt.cm.Blues)
    ax.invert_yaxis()

    # thresh = mat.max() / 2

    def show_values(pc, fmt="%.3f", **kw):
        pc.update_scalarmappable()
        ax = pc.axes
        for p, color, value in itertools.zip_longest(pc.get_paths(), pc.get_facecolors(), pc.get_array()):
            x, y = p.vertices[:-2, :].mean(0)
            if np.all(color[:3] > 0.5):
                color = (0.0, 0.0, 0.0)
            else:
                color = (1.0, 1.0, 1.0)
            ax.text(x, y, fmt % value, ha="center", va="center", color=color, **kw, fontsize=10)

    if if_show_values:
        show_values(im)
    # for i, j in itertools.product(range(mat.shape[0]), range(mat.shape[1])):
    #     ax.text(j, i, "{:.2f}".format(mat[i, j]), fontsize = 4,
    #              horizontalalignment="center",
    #              color="white" if mat[i, j] > thresh else "black")

    plt.tight_layout()
    if pic_fname:
        plt.savefig(pic_fname, dpi=300, transparent=True)
    plt.show()
    plt.close()

"""# Question 16"""

reducer = umap.UMAP(n_components=20, metric='cosine')
X_reduced = reducer.fit_transform(X_tfidfVectors)

hdb = hdbscan.HDBSCAN(min_cluster_size=100)
hdb.fit(X_reduced)
contingencyMatrix = metrics.cluster.contingency_matrix(labels, hdb.labels_)
plot_mat(contingencyMatrix, title="Contingency Matrix for UMAP + HDBSCAN(20 components, cosine)", size=(15,15))

Y_pred_hdb=hdb.labels_

print("Unique Clusters: " + str(np.unique(Y_pred_hdb)))

"""#Question 17 + 18"""

!pip install umap-learn

!pip install hdbscan

from sklearn.cluster import AgglomerativeClustering
from sklearn.pipeline import Pipeline
from sklearn.decomposition import TruncatedSVD
from sklearn.decomposition import NMF
import umap.umap_ as umap # separate
from sklearn.cluster import KMeans
import hdbscan

#scores = pd.DataFrame(columns=['Dim Red', 'Dim Red Param', 'Clustering', 'Clustering Param', 'Homogeneity', 'Completeness', 'V-measure', 'Adjusted Rand-Index','Adjusted Mutual Information'])
from collections import defaultdict
from sklearn import metrics
scores=defaultdict(list)

# Dimensionality reduction: None (0), SVD (1), NMF (2), UMAP (3)
# Clustering: Agglomerative Clustering (1), HDBSCAN (2)
def evaluate(model, dim_red, dim_red_p, clustering, clustering_p):
  preds = model.labels_

  scores["Dim Red"].append(dim_red)
  scores["Dim Red Param"].append(dim_red_p)
  scores["Clustering"].append(clustering)
  scores["Clustering Param"].append(clustering_p)
 
  scores["Homogeneity"].append(metrics.homogeneity_score(labels,preds))
  scores["Completeness"].append(metrics.completeness_score(labels, preds))
  scores["V-measure"].append(metrics.v_measure_score(labels, preds))
  scores["Adjusted Rand-Index"].append(metrics.adjusted_rand_score(labels, preds))
  scores["Adjusted Mutual Information"].append(metrics.adjusted_mutual_info_score(labels, preds))

"""## Dimensionality Reduction: SVD"""

# Dimensionality Reduction = SVD
n_components = [5, 20, 200] 
## Clustering
k_clusters = [10,20,50]
n_clusters=[20] # agg
min_cluster_size = [100,200] #HDBSCAN

for n in n_components:
  svd = TruncatedSVD(n_components=n)
  X_svd = svd.fit_transform(X_tfidfVectors)
  print ("SVD:" + str(n))

  # K-means
  for k in k_clusters:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X_svd)
    evaluate(kmeans,"SVD",n,"Kmeans",k)
    print("K-means:"+str(k))

  # agglomerative clustering
  for n_clust in n_clusters:
    ag = AgglomerativeClustering(n_clusters=n_clust)
    ag.fit(X_svd)
    evaluate(ag, "SVD", n, "Agglomerative", n_clust)
    print("Agg:"+str(n_clust))

  #HDBSCAN
  for m in min_cluster_size:
    hdb = hdbscan.HDBSCAN(min_cluster_size=m)
    hdb.fit(X_svd)
    evaluate(hdb, "SVD", n, "HDBSCAN", m)
    print("HDB:"+str(m))

scores
scoresDF=pd.DataFrame.from_dict(scores)
scoresDF

"""## Dimensionality Reduction: NMF"""

n_components = [5, 20, 200] 
## Clustering
k_clusters = [10,20,50]
n_clusters=[20] # agg
min_cluster_size = [100,200] #HDBSCAN

for n in n_components:
  nmf = NMF(n_components=n)
  X_nmf = nmf.fit_transform(X_tfidfVectors)
  print ("\nNMF:" + str(n))

  # K-means
  for k in k_clusters:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X_nmf)
    evaluate(kmeans,"NMF",n,"Kmeans",k)
    print("K-means:"+str(k))

  # agglomerative clustering
  for n_clust in n_clusters:
    ag = AgglomerativeClustering(n_clusters=n_clust)
    ag.fit(X_nmf)
    evaluate(ag, "NMF", n, "Agglomerative", n_clust)
    print("Agg:"+str(n_clust))

  #HDBSCAN
  for m in min_cluster_size:
    hdb = hdbscan.HDBSCAN(min_cluster_size=m)
    hdb.fit(X_nmf)
    evaluate(hdb, "NMF", n, "HDBSCAN", m)
    print("HDB:"+str(m))

scores
scoresDF=pd.DataFrame.from_dict(scores)
scoresDF

"""## Dimensionality Reduction: UMAP"""

# pickle UMAP
n_components=[5,20,200]

"""### UMAP (5)"""

umap_5 = umap.UMAP(n_components=5, metric='cosine')
X_umap_5 = umap_5.fit_transform(X_tfidfVectors)

# K-means
for k in k_clusters:
  kmeans = KMeans(n_clusters=k)
  kmeans.fit(X_umap_5)
  evaluate(kmeans,"UMAP",5,"Kmeans",k)
  print("K-means:"+str(k))

# agglomerative clustering
for n_clust in n_clusters:
  ag = AgglomerativeClustering(n_clusters=n_clust)
  ag.fit(X_umap_5)
  evaluate(ag, "UMAP", 5, "Agglomerative", n_clust)
  print("Agg:"+str(n_clust))

#HDBSCAN
for m in min_cluster_size:
  hdb = hdbscan.HDBSCAN(min_cluster_size=m)
  hdb.fit(X_umap_5)
  evaluate(hdb, "UMAP", 5, "HDBSCAN", m)
  print("HDB:"+str(m))

scores
scoresDF=pd.DataFrame.from_dict(scores)
scoresDF

"""### UMAP (20)"""

umap_20 = umap.UMAP(n_components=20, metric='cosine')
X_umap_20 = umap_20.fit_transform(X_tfidfVectors)

# K-means
for k in k_clusters:
  kmeans = KMeans(n_clusters=k)
  kmeans.fit(X_umap_20)
  evaluate(kmeans,"UMAP",20,"Kmeans",k)
  print("K-means:"+str(k))

# agglomerative clustering
for n_clust in n_clusters:
  ag = AgglomerativeClustering(n_clusters=n_clust)
  ag.fit(X_umap_20)
  evaluate(ag, "UMAP", 20, "Agglomerative", n_clust)
  print("Agg:"+str(n_clust))

#HDBSCAN
for m in min_cluster_size:
  hdb = hdbscan.HDBSCAN(min_cluster_size=m)
  hdb.fit(X_umap_20)
  evaluate(hdb, "UMAP", 20, "HDBSCAN", m)
  print("HDB:"+str(m))

scores
scoresDF=pd.DataFrame.from_dict(scores)
scoresDF

"""### UMAP (200)"""

umap_200 = umap.UMAP(n_components=200, metric='cosine')
X_umap_200 = umap_200.fit_transform(X_tfidfVectors)

# K-means
for k in k_clusters:
  kmeans = KMeans(n_clusters=k)
  kmeans.fit(X_umap_200)
  evaluate(kmeans,"UMAP",200,"Kmeans",k)
  print("K-means:"+str(k))

# agglomerative clustering
for n_clust in n_clusters:
  ag = AgglomerativeClustering(n_clusters=n_clust)
  ag.fit(X_umap_200)
  evaluate(ag, "UMAP", 200, "Agglomerative", n_clust)
  print("Agg:"+str(n_clust))

#HDBSCAN
for m in min_cluster_size:
  hdb = hdbscan.HDBSCAN(min_cluster_size=m)
  hdb.fit(X_umap_200)
  evaluate(hdb, "UMAP", 200, "HDBSCAN", m)
  print("HDB:"+str(m))

scores
scoresDF=pd.DataFrame.from_dict(scores)
scoresDF

scoresDF.sort_values(by='V-measure', ascending=False)

scoresDF.sort_values(by='Adjusted Rand-Index', ascending=False)

import torch
import torch.nn as nn
from torchvision import transforms, datasets
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt

from tqdm import tqdm
import requests
import os
import tarfile

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix, adjusted_rand_score, adjusted_mutual_info_score
from sklearn.pipeline import Pipeline
from sklearn.base import TransformerMixin

"""# Flowers Dataset and VGG Features

![image.png](attachment:image.png)
"""

filename = './flowers_features_and_labels.npz'

if os.path.exists(filename):
    file = np.load(filename)
    f_all, y_all = file['f_all'], file['y_all']

else:
    if not os.path.exists('./flower_photos'):
        # download the flowers dataset and extract its images
        url = 'http://download.tensorflow.org/example_images/flower_photos.tgz'
        with open('./flower_photos.tgz', 'wb') as file:
            file.write(requests.get(url).content)
        with tarfile.open('./flower_photos.tgz') as file:
            file.extractall('./')
        os.remove('./flower_photos.tgz')

    class FeatureExtractor(nn.Module):
        def __init__(self):
            super().__init__()

            vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)

            # Extract VGG-16 Feature Layers
            self.features = list(vgg.features)
            self.features = nn.Sequential(*self.features)
            # Extract VGG-16 Average Pooling Layer
            self.pooling = vgg.avgpool
            # Convert the image into one-dimensional vector
            self.flatten = nn.Flatten()
            # Extract the first part of fully-connected layer from VGG16
            self.fc = vgg.classifier[0]

        def forward(self, x):
            # It will take the input 'x' until it returns the feature vector called 'out'
            out = self.features(x)
            out = self.pooling(out)
            out = self.flatten(out)
            out = self.fc(out) 
            return out 

    # Initialize the model
    assert torch.cuda.is_available()
    feature_extractor = FeatureExtractor().cuda().eval()

    dataset = datasets.ImageFolder(root='./flower_photos',
                                   transform=transforms.Compose([transforms.Resize(224),
                                                                 transforms.CenterCrop(224),
                                                                 transforms.ToTensor(),
                                                                 transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))
    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

    # Extract features and store them on disk
    f_all, y_all = np.zeros((0, 4096)), np.zeros((0,))
    for x, y in tqdm(dataloader):
        with torch.no_grad():
            f_all = np.vstack([f_all, feature_extractor(x.cuda()).cpu()])
            y_all = np.concatenate([y_all, y])
    np.savez(filename, f_all=f_all, y_all=y_all)

"""#Question 21"""

print(f_all.shape, y_all.shape)
num_features = f_all.shape[1]
print(num_features)

"""#Question 22"""

f_pca = PCA(n_components=2).fit_transform(f_all)
plt.scatter(*f_pca.T, c=y_all)

"""# Question 23"""

from sklearn.manifold import TSNE

# map the extracted features onto 2 dimensions with t-SNE

# plot mapped feature vectors along x and y axes
# color code the data points with ground-truth labels
tsne = TSNE(n_components=2)
X_tsne = tsne.fit_transform(f_all)
X_tsne.shape

plt.scatter(*X_tsne.T, c=y_all) # the '*' operator unpacks the contents of X_tsne
# the c=y_all sets the color of each point in the scatter plot based on the ground truth y_all ables

"""#Question 24"""

!pip install umap-learn

!pip install hdbscan

import umap.umap_ as umap

from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
import hdbscan



from sklearn.cluster import AgglomerativeClustering
from hdbscan import HDBSCAN

# kmeans
print("K means")
kmeans = KMeans(n_clusters=5, random_state=0, max_iter=1000, n_init=30).fit(f_all)
report(kmeans.labels_, y_all)

# Agglomerative
print("Agglomerative Clustering")
agg_cluster = AgglomerativeClustering(n_clusters=5, linkage='ward').fit(f_all)
report(agg_cluster.labels_, y_all)

# HDBSCAN
print("HDBSCAN")
clusterer = HDBSCAN(min_cluster_size=20).fit(f_all)
report(clusterer.labels_, y_all)

from sklearn.decomposition import TruncatedSVD

reduced = TruncatedSVD(n_components=50).fit_transform(f_all)

# kmeans
print("K means")
kmeans = KMeans(n_clusters=5, random_state=0, max_iter=1000, n_init=30).fit(reduced)
report(kmeans.labels_, y_all)

# Agglomerative
print("Agglomerative Clustering")
agg_cluster = AgglomerativeClustering(n_clusters=5, linkage='ward').fit(reduced)
report(agg_cluster.labels_, y_all)

# HDBSCAN
print("HDBSCAN")
clusterer = HDBSCAN(min_cluster_size=20).fit(reduced)
report(clusterer.labels_, y_all)

reducer = umap.UMAP(metric='cosine', n_components=50)
reduced = reducer.fit_transform(f_all)

# kmeans
print("K means")
kmeans = KMeans(n_clusters=5, random_state=0, max_iter=1000, n_init=30).fit(reduced)
report(kmeans.labels_, y_all)

# Agglomerative
print("Agglomerative Clustering")
agg_cluster = AgglomerativeClustering(n_clusters=5, linkage='ward').fit(reduced)
report(agg_cluster.labels_, y_all)

# HDBSCAN
print("HDBSCAN")
clusterer = HDBSCAN(min_cluster_size=20).fit(reduced)
report(clusterer.labels_, y_all)

reduced = Autoencoder(50).fit_transform(f_all)

# kmeans
print("K means")
kmeans = KMeans(n_clusters=5, random_state=0, max_iter=1000, n_init=30).fit(reduced)
report(kmeans.labels_, y_all)

# Agglomerative
print("Agglomerative Clustering")
agg_cluster = AgglomerativeClustering(n_clusters=5, linkage='ward').fit(reduced)
report(agg_cluster.labels_, y_all)

# HDBSCAN
print("HDBSCAN")
clusterer = HDBSCAN(min_cluster_size=20).fit(reduced)
report(clusterer.labels_, y_all)

"""# Question 25

## MLP Classifier
"""

class MLP(torch.nn.Module):
    def __init__(self, num_features):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(num_features, 1280),
            nn.ReLU(True),
            nn.Linear(1280, 640),
            nn.ReLU(True), 
            nn.Linear(640, 5),
            nn.LogSoftmax(dim=1)
        )
        self.cuda()
    
    
    def forward(self, X):
        return self.model(X)
    
    def train(self, X, y):
        X = torch.tensor(X, dtype=torch.float32, device='cuda')
        y = torch.tensor(y, dtype=torch.int64, device='cuda')

        self.model.train()
        
        criterion = nn.NLLLoss()
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)

        dataset = TensorDataset(X, y)
        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)

        for epoch in tqdm(range(100)):
            for (X_, y_) in dataloader:
                preds = self.model(X_)
                loss = criterion(preds, y_)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
        return self
    
    def eval(self, X_test, y_test):
        X = torch.tensor(X_test, dtype=torch.float32, device='cuda')
        y = torch.tensor(y_test, dtype=torch.int64, device='cuda')

        self.model.eval()
        with torch.no_grad():
          test_preds = torch.argmax(self.model(X), dim=1)
        return torch.sum(test_preds == y) / test_preds.shape[0]

print(f_all.shape, y_all.shape)

"""## Autoencoder"""

class Autoencoder(torch.nn.Module, TransformerMixin):
    def __init__(self, n_components):
        super().__init__()
        self.n_components = n_components
        self.n_features = None  # to be determined with data
        self.encoder = None
        self.decoder = None
        
    def _create_encoder(self):
        return nn.Sequential(
            nn.Linear(4096, 1280),
            nn.ReLU(True),
            nn.Linear(1280, 640),
            nn.ReLU(True), nn.Linear(640, 120), nn.ReLU(True), nn.Linear(120, self.n_components))
    
    def _create_decoder(self):
        return nn.Sequential(
            nn.Linear(self.n_components, 120),
            nn.ReLU(True),
            nn.Linear(120, 640),
            nn.ReLU(True),
            nn.Linear(640, 1280),
            nn.ReLU(True), nn.Linear(1280, 4096))
    
    def forward(self, X):
        encoded = self.encoder(X)
        decoded = self.decoder(encoded)
        return decoded
    
    def fit(self, X):
        X = torch.tensor(X, dtype=torch.float32, device='cuda')
        self.n_features = X.shape[1]
        self.encoder = self._create_encoder()
        self.decoder = self._create_decoder()
        self.cuda()
        self.train()
        
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, weight_decay=1e-5)

        dataset = TensorDataset(X)
        dataloader = DataLoader(dataset, batch_size=128, shuffle=True)

        for epoch in tqdm(range(100)):
            for (X_,) in dataloader:
                X_ = X_.cuda()
                # ===================forward=====================
                output = self(X_)
                loss = criterion(output, X_)
                # ===================backward====================
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

        return self     
        
    def transform(self, X):
        X = torch.tensor(X, dtype=torch.float32, device='cuda')
        self.eval()
        with torch.no_grad():
            return self.encoder(X).cpu().numpy()

X_em =Autoencoder(2).fit_transform(f_all)
plt.scatter(*X_em.T, c=y_all)

from sklearn.model_selection import train_test_split

f_train, f_test, y_train, y_test = train_test_split(f_all, y_all, test_size=0.20, random_state=0)
print(f_train.shape, f_test.shape, y_train.shape, y_test.shape)

mlp = MLP(4096).train(f_train, y_train)
print(mlp.eval(f_test, y_test))

from sklearn.decomposition import TruncatedSVD

reduced = TruncatedSVD(n_components=50).fit_transform(f_all)

f_train, f_test, y_train, y_test = train_test_split(reduced, y_all, test_size=0.20, random_state=0)
print(f_train.shape, f_test.shape, y_train.shape, y_test.shape)

mlp = MLP(50).train(f_train, y_train)
print(mlp.eval(f_test, y_test))

reducer = umap.UMAP(metric='cosine', n_components=50)
reduced = reducer.fit_transform(f_all)

f_train, f_test, y_train, y_test = train_test_split(reduced, y_all, test_size=0.20, random_state=0)
print(f_train.shape, f_test.shape, y_train.shape, y_test.shape)

mlp = MLP(50).train(f_train, y_train)
print(mlp.eval(f_test, y_test))

reduced = Autoencoder(50).fit_transform(f_all)

f_train, f_test, y_train, y_test = train_test_split(reduced, y_all, test_size=0.20, random_state=0)
print(f_train.shape, f_test.shape, y_train.shape, y_test.shape)

mlp = MLP(50).train(f_train, y_train)
print(mlp.eval(f_test, y_test))

eps_rec = []
min_samples_rec = []
db_hs = []
db_cs = []
db_vs = []
db_ari = []
db_ms = []

eps = [0.01, 0.1,1.0,5.0,10.0,50.0]
min_samples = [5,15,30,60,100,200,500]

for ep in eps:
    for samp in min_samples:
      dbs = DBSCAN(eps=ep,min_samples=samp)
      dbs_svd = agglo_ward.fit(svd_f)
      dbs_umap = agglo_ward.fit(umap_cos_f)
      dbs_x_em = agglo_ward.fit(x_em_f)
      print(f'--- EP: {ep} & Min Sample = {samp}---')
      print("SVD + DBSCAN RAND SCORE : ", adjusted_rand_score(y_all, dbs_svd.labels_))
      print("UMAP + DBSCAN RAND SCORE : ", adjusted_rand_score(y_all, dbs_umap.labels_))
      print("Auto Encoder + DBSCAN RAND SCORE : ", adjusted_rand_score(y_all, dbs_x_em.labels_))